import os
from dotenv import load_dotenv
import streamlit as st
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain.chains import ConversationalRetrievalChain
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from tempfile import NamedTemporaryFile
from langchain.prompts import PromptTemplate

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()
openai_api_key = os.getenv("OPENAI_API_KEY")

# Streamlit í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="PDF RAG ì±—ë´‡", layout="wide")
st.title("ğŸ“„ PDF ê¸°ë°˜ RAG ì±—ë´‡")

if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

# PDF ì—…ë¡œë“œ
uploaded_file = st.file_uploader("PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”", type=["pdf"])

if uploaded_file:
    with NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
        tmp_file.write(uploaded_file.read())
        tmp_path = tmp_file.name

    # PDF ë¬¸ì„œ ë¡œë”©
    loader = PyPDFLoader(tmp_path)
    pages = loader.load()

    # [TODO] ì²­í¬ ì‚¬ì´ì¦ˆì™€ ì˜¤ë²„ë©ì„ ë°”ê¿”ê°€ë©° RAG ì±—ë´‡ì˜ ì„±ëŠ¥ ë³€í™”ë¥¼ í™•ì¸í•´ ë³´ì„¸ìš”.
    splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=100)
    docs = splitter.split_documents(pages)

    # ë²¡í„° ì €ì¥ì†Œ
    embeddings = OpenAIEmbeddings(api_key=openai_api_key, model="text-embedding-3-small")
    vectorstore = FAISS.from_documents(docs, embeddings)

    # Retriever êµ¬ì„±
    retriever = vectorstore.as_retriever(search_type="similarity", search_kwargs={"k": 5})

    # LLM ë° ì‹œìŠ¤í…œ ë©”ì‹œì§€ í¬í•¨í•œ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.2, api_key=openai_api_key)

    # [TODO] ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¥¼ ë°”ê¿”ê°€ë©° RAG ì±—ë´‡ì˜ ì„±ëŠ¥ ë³€í™”ë¥¼ í™•ì¸í•´ ë³´ì„¸ìš”.
    prompt_template = """
        ë‹¹ì‹ ì€ ì—…ë¡œë“œëœ PDF ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ì •ì¤‘í•˜ê³  ê°„ê²°í•˜ê²Œ ì‘ë‹µí•˜ëŠ” AIì…ë‹ˆë‹¤.
        ë¬¸ì„œì—ì„œ ì§ì ‘ì ì¸ ë‹µì„ ì°¾ì„ ìˆ˜ ì—†ì„ ê²½ìš°, ê°€ëŠ¥í•œ í•œ ìœ ì‚¬í•œ ì •ë³´ë¥¼ ì œê³µí•˜ê³  ì¶”ì¸¡ì´ ì•„ë‹Œ ì •ë³´ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•´ì•¼ í•©ë‹ˆë‹¤.

        ì§ˆë¬¸: {question}
        ì´ì „ ëŒ€í™” ë‚´ìš©: {chat_history}

        ì •ì¤‘í•˜ê³  ì •í™•í•˜ê²Œ ëŒ€ë‹µí•˜ì„¸ìš”.
    """

    prompt = PromptTemplate(
        input_variables=["question", "chat_history"],
        template=prompt_template.strip()
    )

    qa_chain = ConversationalRetrievalChain.from_llm(
        llm=llm,
        retriever=retriever,
        condense_question_prompt=prompt,
        return_source_documents=True
    )

    # ì±„íŒ… UI
    for q, a in st.session_state.chat_history:
        with st.chat_message("user"):
            st.markdown(q)
        with st.chat_message("assistant"):
            st.markdown(a)

    user_input = st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”")

    if user_input:
        with st.chat_message("user"):
            st.markdown(user_input)

        # ì´ì „ ì§ˆë¬¸ ëª©ë¡ë§Œ ë„˜ê¹€ (ì§ˆë¬¸ë§Œ ì¶”ì¶œ)
        chat_history = [(q, a) for q, a in st.session_state.chat_history]

        result = qa_chain(
            {"question": user_input, "chat_history": chat_history}
        )
        answer = result["answer"]

        with st.chat_message("assistant"):
            st.markdown(answer)

        st.session_state.chat_history.append((user_input, answer))

else:
    st.info("ë¨¼ì € PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.")
